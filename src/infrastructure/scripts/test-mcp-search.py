#!/usr/bin/env python3
"""
@file: test-mcp-search.py
@description: –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–∞ MCP —Å–µ—Ä–≤–µ—Ä–æ–≤
@dependencies: cursor-directory-scraper.py
@created: 2025-01-27
@pager-protection: –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç pager –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
"""

import asyncio
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é scraper
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ —Ñ–∞–π–ª–∞
import importlib.util
spec = importlib.util.spec_from_file_location("cursor_directory_scraper", "cursor-directory-scraper.py")
cursor_directory_scraper = importlib.util.module_from_spec(spec)
spec.loader.exec_module(cursor_directory_scraper)

async def test_search():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ MCP —Å–µ—Ä–≤–µ—Ä–æ–≤"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ MCP —Å–µ—Ä–≤–µ—Ä–æ–≤ –Ω–∞ cursor.directory")
    print("=" * 60)
    
    scraper = cursor_directory_scraper.CursorDirectoryScraper()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "terminal",
        "java", 
        "database",
        "email"
    ]
    
    for query in test_queries:
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫: '{query}'")
        print("-" * 40)
        
        try:
            servers = await scraper.search_mcp_servers(query)
            
            if servers:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(servers)} —Å–µ—Ä–≤–µ—Ä–æ–≤")
                print("\nüìã –¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
                
                for i, server in enumerate(servers[:5], 1):
                    print(f"{i}. {server['title']}")
                    if server.get('link'):
                        print(f"   üîó {server['link']}")
                    if server.get('description'):
                        desc = server['description'][:100] + "..." if len(server['description']) > 100 else server['description']
                        print(f"   üìù {desc}")
                    print()
            else:
                print("‚ùå –°–µ—Ä–≤–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
    
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞:")
    stats = scraper.get_cache_stats()
    print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {stats['total_files']}")
    print(f"   –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {stats['total_size_mb']} MB")
    print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {len(stats['cached_queries'])}")

if __name__ == "__main__":
    asyncio.run(test_search())
